{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amir3x0/LLM-mini_projects/blob/main/Hugging_Face_HowToUse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "extwY1w8fZTH"
      },
      "source": [
        "- Hugging Face: https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdXxhkKurTnz"
      },
      "source": [
        "# INSTALL KEY LIBRARIES, OBTAIN HF ACCESS TOKENS, & GPU CHECK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V7GS16yxrKl"
      },
      "source": [
        "**Installing Libraries:**\n",
        "\n",
        "*   `transformers`: The core Hugging Face library for models and tokenizers.\n",
        "*   `accelerate`: Helps run models efficiently across different hardware (like GPUs) and use less memory.\n",
        "*   `bitsandbytes`: Enables model quantization (like loading in 4-bit or 8-bit), drastically reducing memory usage. Essential for running decent models on free Colab GPUs!\n",
        "*   `torch`: The underlying deep learning framework (PyTorch).\n",
        "*   `pypdf`: A library to easily extract text from PDF files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN9dJNO0xrKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7701a244-247d-45c2-adb5-9208d3259f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing necessary libraries...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLibraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing necessary libraries...\")\n",
        "!pip install -q transformers accelerate bitsandbytes torch pypdf gradio\n",
        "print(\"Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUYFEk6dqZ19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d99abfd-6e46-4493-8ce5-e8f4fcd3ebb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core libraries imported.\n"
          ]
        }
      ],
      "source": [
        "import torch  # PyTorch, the backend for transformers\n",
        "import pypdf  # For reading PDFs\n",
        "import gradio as gr  # For building the UI\n",
        "from IPython.display import display, Markdown  # For nicer printing in notebooks\n",
        "print(\"Core libraries imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLCXZD5mxrKl"
      },
      "source": [
        "**Hugging Face Hub Login Steps:**\n",
        "\n",
        "    1.  Go to [huggingface.co](https://huggingface.co/).\n",
        "    2.  Sign up or log in.\n",
        "    3.  profile picture (top right) -> Settings -> Access Tokens.\n",
        "    4.  Create a new token (a 'read' role is usually sufficient).\n",
        "    5.  Copy the generated token. **Treat this like a password!**\n",
        "*   **Log in within Colab:** We'll use a helper function from the `huggingface_hub` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEvNhww8xrKm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53,
          "referenced_widgets": [
            "eaacaa044fbb4d41a429aded65f1d598",
            "35698f748c634bfd8a8375dbcdd2b5bc",
            "b7860e5a0f42464db5bbf2cb3f13bfce",
            "9940891b55664eb7915cba801d9e5d97",
            "583587adb2fa43fc8113c65327011d40",
            "1dcf3a096aee4d80bf60f7ab2523dfd2",
            "eb2ecc44a404404dae0e88ac81ee30e9",
            "fd54263dd4874d4d8762eb3b71112170",
            "2558f22ce7064557ba9b1eb8f7d9504a",
            "fe4c873e5a174b328928529b8926d4d9",
            "5a971d4c8cb547afb2c91c0b3d6ba6e9",
            "02568b198c994b4cbf309ba57651797a",
            "9734b9bce11e4ca29423e5842c560bda",
            "4b17a157a57e4c3c9d9f9b3978fa0d48",
            "a1ca5b145ec14b3fba77d1848bbeb957",
            "ef1072e844a74341b83355f55e0b9ecf",
            "fe00368e5c43489fb5d64a266da020e1",
            "05f51f3ea8e94bfb89c5354945ffc821",
            "59b383c7793546abb87ab5d28234dfc4",
            "e75a70e169a44465bc938a0741684030"
          ]
        },
        "outputId": "69c19e2e-91d1-48ae-8113-ea17273bf4cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting Hugging Face login...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaacaa044fbb4d41a429aded65f1d598"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Login successful (or token already present)!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from huggingface_hub import login, notebook_login\n",
        "print(\"Attempting Hugging Face login...\")\n",
        "\n",
        "notebook_login()\n",
        "print(\"Login successful (or token already present)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3IgGTK7s7n5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9488f4-c574-4134-f219-7036dffff147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU detected: Tesla T4\n",
            "PyTorch default device set to CUDA (GPU).\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    # Set default device to GPU\n",
        "    torch.set_default_device(\"cuda\")\n",
        "    print(\"PyTorch default device set to CUDA (GPU).\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Running these models on CPU will be extremely slow!\")\n",
        "    print(\"Make sure 'GPU' is selected in Runtime > Change runtime type.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px5QHvVQsbaC"
      },
      "outputs": [],
      "source": [
        "# Helper function for markdown display\n",
        "def print_markdown(text):\n",
        "    \"\"\"Displays text as Markdown in Colab\"\"\"\n",
        "    display(Markdown(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fcPYViht5bz"
      },
      "outputs": [],
      "source": [
        "# pipelines - easy way to use models for inference.\n",
        "# These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks\n",
        "# Those tasks include Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a sentiment classifier model on financial news data\n",
        "# Check the model here: https://huggingface.co/ProsusAI/finbert\n",
        "pipe = pipeline(model = \"ProsusAI/finbert\")\n",
        "pipe(\"Apple lost 10 Million dollars today due to US tarrifs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVwoex5qxG4J"
      },
      "outputs": [],
      "source": [
        "# A tokenizer converts text into numerical IDs that the model understands\n",
        "# Check a demo for OpenAI's Tokenizers here: https://platform.openai.com/tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer for GPT-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Encode text to token IDs\n",
        "tokens = tokenizer(\"Hello, I am studying LLM and AI Agent more in depth.\")\n",
        "print(tokens['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NveqDFvUGih8"
      },
      "source": [
        "# HUGGING FACE TRANSFORMERS LIBRARY: AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS4X4KzfxrKm"
      },
      "source": [
        "AutoModelForCausalLM is a Hugging Face class that automatically loads a pretrained model for causal (left-to-right) language modeling, such as GPT, LLaMA, or Gemma.\n",
        "\n",
        "Let's get hands-on and load a model! We'll start with a relatively small but capable model that should fit comfortably in Colab's free tier GPU memory, thanks to quantization.\n",
        "\n",
        "**Key Steps:**\n",
        "\n",
        "1.  **Choose a Model ID:** We need the unique identifier from the Hugging Face Hub (e.g., `\"google/gemma-2b-it\"` or `\"microsoft/Phi-3-mini-4k-instruct\"`).\n",
        "2.  **Load the Tokenizer:** Use `AutoTokenizer.from_pretrained(model_id)` to get the specific tokenizer for that model.\n",
        "3.  **Load the Model:** Use `AutoModelForCausalLM.from_pretrained(...)` with crucial arguments:\n",
        "    *   `model_id`: The identifier.\n",
        "    *   `torch_dtype=torch.float16` (or `bfloat16`): Loads the model using 16-bit floating point numbers instead of 32-bit, saving memory.\n",
        "    *   `load_in_4bit=True` or `load_in_8bit=True`: This is **quantization** via `bitsandbytes`. It further reduces memory by representing model weights with fewer bits (4 or 8 instead of 16/32). Essential for free Colab! 4-bit saves more memory but might have a tiny impact on quality compared to 8-bit.\n",
        "    *   `device_map=\"auto\"`: Tells `accelerate` to automatically figure out how to spread the model across available devices (primarily the GPU in our case).\n",
        "4.  **Combine Tokenizer and Model (Optional but common):** Using the `pipeline` function is often simpler for basic text generation. It handles tokenization, model inference, and decoding back to text for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zUl_7FxgWLN"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_jF8eoCxrKm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Alternatives models to try (might need login/agreement):\n",
        "# model_id = \"unsloth/gemma-3-4b-it-GGUF\"\n",
        "# model_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "# model_id = \"unsloth/Llama-3.2-3B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8G7_zk3xrKm"
      },
      "outputs": [],
      "source": [
        "# load the Tokenizer\n",
        "# The tokenizer prepares text input for the model\n",
        "# trust_remote_code=True is sometimes needed for newer models with custom code.\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)\n",
        "print(\"Tokenizer loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iieH9AY8xrKm"
      },
      "outputs": [],
      "source": [
        "# Let's Load the Model with Quantization\n",
        "\n",
        "print(f\"Loading model: {model_id}\")\n",
        "print(\"This might take a few minutes, especially the first time...\")\n",
        "\n",
        "# Create BitsAndBytesConfig for 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit = True,\n",
        "                                         bnb_4bit_compute_dtype = torch.float16,  # or torch.bfloat16 if available\n",
        "                                         bnb_4bit_quant_type = \"nf4\",  # normal float 4 quantization\n",
        "                                         bnb_4bit_use_double_quant = True  # use nested quantization for more efficient memory usage\n",
        "                                         )\n",
        "\n",
        "# Load the model with the quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             quantization_config = quantization_config,\n",
        "                                             device_map = \"auto\",\n",
        "                                             trust_remote_code = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FR8-VWK-5xB"
      },
      "outputs": [],
      "source": [
        "prompt = \"Explain how Electric Vehicles work in a funny way!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXQw5pajmlBn"
      },
      "outputs": [],
      "source": [
        "prompt = \"What is the capital of France?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKwextto8ktb"
      },
      "outputs": [],
      "source": [
        "# Method 1: test the model and Tokenizer using the .generate() method!\n",
        "\n",
        "# encode the input first\n",
        "inputs = tokenizer(prompt, return_tensors = \"pt\")\n",
        "\n",
        "# generate the output\n",
        "outputs = model.generate(**inputs, max_new_tokens = 1000)\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print_markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhXfJt-lueTq"
      },
      "outputs": [],
      "source": [
        "# Method 2: create a pipeline that includes your model and tokenizer\n",
        "# The pipeline wraps tokenization, generation, and decoding\n",
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model = model,\n",
        "                tokenizer = tokenizer,\n",
        "                torch_dtype = \"auto\", # Match model dtype\n",
        "                device_map = \"auto\" # Ensure pipeline uses the same device mapping\n",
        "                )\n",
        "\n",
        "\n",
        "outputs = pipe(prompt,\n",
        "               max_new_tokens = 1000, # max_new_tokens limits the length of the generated response.\n",
        "               temperature = 1, # temperature controls randomness (lower = more focused).\n",
        "               )\n",
        "\n",
        "# Print the generated text\n",
        "print_markdown(outputs[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-hx96rPxJQx"
      },
      "source": [
        "# READ PDF DOCUMENTS & EXTRACT TEXT USING PYPDF LIBRARY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCt5k08VxrKn"
      },
      "source": [
        "**Steps:**\n",
        "1.  **Get the PDF:** Download it or specify the path if uploaded.\n",
        "2.  **Open the PDF:** Use `pypdf.PdfReader`.\n",
        "3.  **Iterate Through Pages:** Loop through each page in the PDF.\n",
        "4.  **Extract Text:** Use `page.extract_text()`.\n",
        "5.  **Combine Text:** Join the text from all pages into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_1zMeknxrKn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Get the PDF File ---\n",
        "pdf_url = \"https://abc.xyz/assets/66/ae/c94682fc4137b5fb90a5d709ac4b/2025-q1-earnings-transcript.pdf\"\n",
        "pdf_filename = \"google_earning_transcript.pdf\"\n",
        "pdf_path = Path(pdf_filename)\n",
        "\n",
        "# Download the file if it doesn't exist\n",
        "if not pdf_path.exists():\n",
        "    response = requests.get(pdf_url)\n",
        "    response.raise_for_status()  # Check for download errors\n",
        "    pdf_path.write_bytes(response.content)\n",
        "    print(f\"PDF downloaded successfully to {pdf_path}\")\n",
        "else:\n",
        "    print(f\"PDF file already exists at {pdf_path}\")\n",
        "\n",
        "\n",
        "# --- Read Text from PDF using pypdf ---\n",
        "pdf_text = \"\"\n",
        "\n",
        "print(f\"Reading text from {pdf_path}...\")\n",
        "reader = pypdf.PdfReader(pdf_path)\n",
        "num_pages = len(reader.pages)\n",
        "print(f\"PDF has {num_pages} pages.\")\n",
        "\n",
        "# Extract text from each page\n",
        "all_pages_text = []\n",
        "for i, page in enumerate(reader.pages):\n",
        "\n",
        "    page_text = page.extract_text()\n",
        "    if page_text:  # Only add if text extraction was successful\n",
        "        all_pages_text.append(page_text)\n",
        "    # print(f\"Read page {i+1}/{num_pages}\") # Uncomment for progress\n",
        "\n",
        "# Join the text from all pages\n",
        "pdf_text = \"\\n\".join(all_pages_text)\n",
        "print(f\"Successfully extracted text. Total characters: {len(pdf_text)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UDa0U7O9xjH_",
        "outputId": "beafb3b7-8b64-45eb-bd6b-64140bd8b33a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Snippet of Extracted Text ---\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              " \n",
              "This  transcript  is  provided  for  the  convenience  of  investors  only,  for  a  full  recording  please  see  the  Q1  2025  Earnings  Call  webcast.    Operator:  Welcome,  everyone.  Thank  you  for  standing  by  for  the  Alphabet  First  Quarter  2025  \n",
              "Earnings\n",
              " \n",
              "conference\n",
              " \n",
              "call.\n",
              " \n",
              "  At  this  time,  all  participants  are  in  a  listen-only  mode.  After  the  speaker  presentations,  there  will  \n",
              "be\n",
              " \n",
              "a\n",
              " \n",
              "question-and-answer\n",
              " \n",
              "session.\n",
              " \n",
              "To\n",
              " \n",
              "ask\n",
              " \n",
              "a\n",
              " \n",
              "question\n",
              " \n",
              "during\n",
              " \n",
              "the\n",
              " \n",
              "session,\n",
              " \n",
              "you\n",
              " \n",
              "will\n",
              " \n",
              "need\n",
              " \n",
              "to\n",
              " \n",
              "press\n",
              " \n",
              "*1\n",
              " \n",
              "on\n",
              " \n",
              "your\n",
              " \n",
              "telephone.\n",
              "  I  would  now  like  to  hand  the  conference  over  to  your  speaker  today,  Jim  Friedland,  Senior  \n",
              "Director\n",
              " \n",
              "of\n",
              " \n",
              "Investor\n",
              " \n",
              "Relations.\n",
              " \n",
              "Please\n",
              " \n",
              "go\n",
              " \n",
              "ahead.\n",
              " \n",
              "  Jim  Friedland,  Senior  Director,  Investor  Relations:  Thank  you.  Good  afternoon,  everyone,  \n",
              "and\n",
              " \n",
              "welcome\n",
              " \n",
              "to\n",
              " \n",
              "Alphabet's\n",
              " \n",
              "First\n",
              " \n",
              "Quarter\n",
              " \n",
              "2025\n",
              " \n",
              "Earnings\n",
              " \n",
              "Conference\n",
              " \n",
              "Call.\n",
              " \n",
              "With\n",
              " \n",
              "us\n",
              " \n",
              "today\n",
              " \n",
              "are\n",
              " \n",
              "Sundar\n",
              " \n",
              "Pichai,\n",
              " \n",
              "Philipp\n",
              " \n",
              "Schin"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display a small snippet of the PDF\n",
        "print(\"\\n--- Snippet of Extracted Text ---\")\n",
        "print_markdown(f\"{pdf_text[:1000]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-lwRZfDx63R"
      },
      "source": [
        "# BUILD THE Q&A LOGIC & PROMPT THE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adPMVre_xrKn"
      },
      "source": [
        "two key ingredients:\n",
        "1.  A loaded open-source LLM (and its tokenizer/pipeline).\n",
        "2.  The text content extracted from our PDF document.\n",
        "\n",
        "combine these to answer user questions. The core idea is **prompt engineering**: I will create a prompt that includes both the user's question and the relevant document context, instructing the model to answer based only on that context.\n",
        "\n",
        "**Steps:**\n",
        "1.  **Define a Prompt Template:** Create a string that structures the input for the LLM. This typically includes placeholders for the context (PDF text) and the question.\n",
        "2.  **Create an Answering Function:** Write a Python function that takes the PDF text, the user question, and the model/tokenizer (or pipeline) as input.\n",
        "3.  **Format the Prompt:** Inside the function, fill the template with the actual PDF text and question.\n",
        "4.  **Handle Context Length:** LLMs have a maximum context window (how much text they can read at once). Our sample PDF might be too long! For simplicity now, we might just truncate the PDF text if it's excessive. More advanced techniques involve chunking the document and retrieving only relevant parts, but we'll keep it basic here.\n",
        "5.  **Run Inference:** Send the formatted prompt to the model pipeline.\n",
        "6.  **Extract the Answer:** Process the model's output to get just the answer part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAGe2Yq4xrKn"
      },
      "outputs": [],
      "source": [
        "# Define a limit for the context length to avoid overwhelming the model\n",
        "\n",
        "MAX_CONTEXT_CHARS = 6000\n",
        "\n",
        "def answer_question_from_pdf(document_text, question, llm_pipeline):\n",
        "    \"\"\"\n",
        "    Answers a question based on the provided document text using the loaded LLM pipeline.\n",
        "\n",
        "    Args:\n",
        "        document_text (str): The text extracted from the PDF.\n",
        "        question (str): The user's question.\n",
        "        llm_pipeline (transformers.pipeline): The initialized text-generation pipeline.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's generated answer.\n",
        "    \"\"\"\n",
        "    # Truncate context if necessary\n",
        "    if len(document_text) > MAX_CONTEXT_CHARS:\n",
        "        print(f\"Warning: Document text ({len(document_text)} chars) exceeds limit ({MAX_CONTEXT_CHARS} chars). Truncating.\")\n",
        "        context = document_text[:MAX_CONTEXT_CHARS] + \"...\"\n",
        "    else:\n",
        "        context = document_text\n",
        "\n",
        "    # Prompt Template\n",
        "    # instruct the model to use only the provided document.\n",
        "    # <|system|> provides context/instructions, <|user|> is the question.\n",
        "    # Note: Different models might prefer different prompt structures.\n",
        "    prompt_template = f\"\"\"<|system|>\n",
        "    You are an AI assistant. Answer the following question based *only* on the provided document text. If the answer is not found in the document, say \"The document does not contain information on this topic.\" Do not use any prior knowledge.\n",
        "\n",
        "    Document Text:\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "    <|end|>\n",
        "    <|user|>\n",
        "    Question: {question}<|end|>\n",
        "    <|assistant|>\n",
        "    Answer:\"\"\" # prompt the model to start generating the answer\n",
        "\n",
        "    print(f\"\\n--- Generating Answer for: '{question}' ---\")\n",
        "\n",
        "    # Run Inference on the chosen model\n",
        "    outputs = llm_pipeline(prompt_template,\n",
        "                           max_new_tokens = 500,  # Limit answer length\n",
        "                           do_sample = True,\n",
        "                           temperature = 0.2,   # Lower temperature for more factual Q&A\n",
        "                           top_p = 0.9)\n",
        "\n",
        "    # Let's extract the answer\n",
        "    # The output includes the full prompt template. We need the text generated *after* it.\n",
        "    full_generated_text = outputs[0]['generated_text']\n",
        "    answer_start_index = full_generated_text.find(\"Answer:\") + len(\"Answer:\")\n",
        "    raw_answer = full_generated_text[answer_start_index:].strip()\n",
        "\n",
        "    # Sometimes the model might still include parts of the prompt or trail off.\n",
        "    # Basic cleanup: Find the end-of-sequence token if possible, or just return raw.\n",
        "    # Phi-3 uses <|end|> or <|im_end|>\n",
        "    end_token = \"<|end|>\"\n",
        "    if end_token in raw_answer:\n",
        "            raw_answer = raw_answer.split(end_token)[0]\n",
        "\n",
        "    print(\"--- Generation Complete ---\")\n",
        "    return raw_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "BguZFFmqycjq",
        "outputId": "4b077343-974a-4fbc-98ce-b10de517776c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Document text (64652 chars) exceeds limit (6000 chars). Truncating.\n",
            "\n",
            "--- Generating Answer for: 'What is this document about?' ---\n",
            "--- Generation Complete ---\n",
            "\n",
            "Test Question:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Q:** What is this document about?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Answer:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**A:** This document is about Alphabet's First Quarter 2025 Earnings Conference Call, where Jim Friedland, Senior Director of Investor Relations, presents the company's financial performance and forward-looking statements. CEO Sundar Pichai discusses the company's growth in various business areas, including AI and cloud services, and highlights the progress in AI infrastructure and research. The document also mentions the release of Gemini 2.5 Pro, a state-of-the-art AI model, and the introduction of 2.5 Flash for developers."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Let's test the function\n",
        "test_question = \"What is this document about?\"\n",
        "generated_answer = answer_question_from_pdf(pdf_text, test_question, pipe)\n",
        "\n",
        "print(\"\\nTest Question:\")\n",
        "print_markdown(f\"**Q:** {test_question}\")\n",
        "print(\"\\nGenerated Answer:\")\n",
        "print_markdown(f\"**A:** {generated_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qChwJtTdxrKn"
      },
      "source": [
        "# SWITCH MODELS & BUILD GRADIO INTERFACE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e82sZSGAxrKn"
      },
      "source": [
        "load one model at a time based on the user's selection in the Gradio interface. This means unloading the previous model before loading the new one. This will introduce a loading delay when switching models, but it's necessary for memory constraints.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1.  **Define Model Choices:** Create a dictionary mapping user-friendly names (e.g., \"Phi-3 Mini\") to their Hugging Face model IDs. Include models known to work in Colab free tier with 4-bit quantization.\n",
        "2.  **Global State:** Keep track of the currently loaded model and tokenizer globally (or using Gradio's `State`).\n",
        "3.  **Model Loading Function:** Create a function `load_model(model_id)` that handles unloading the old model (if any) and loading the new tokenizer and quantized model. It should return the new `pipeline`.\n",
        "4.  **Gradio Interface:**\n",
        "    *   Use `gr.Blocks` for more layout control.\n",
        "    *   Add a `gr.Dropdown` for the user to select the desired model.\n",
        "    *   Add a `gr.Textbox` for the user's question.\n",
        "    *   Add a `gr.Textbox` (or `gr.Markdown`) for the output answer.\n",
        "    *   Add a `gr.Button` to submit the question.\n",
        "5.  **Event Handling:**\n",
        "    *   When the dropdown selection changes, trigger the `load_model` function. Show a loading indicator.\n",
        "    *   When the submit button is clicked, call our `answer_question_from_pdf` function, passing the current PDF text, the question, and the currently loaded pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNFogLwOxrKn"
      },
      "outputs": [],
      "source": [
        "# Make sure we have the pdf_text\n",
        "# Configuration: Models available for selection\n",
        "# Use models known to fit in Colab free tier with 4-bit quantization\n",
        "\n",
        "available_models = {\n",
        "    \"Llama 3.2\": \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    \"Microsoft Phi-4 Mini\": \"microsoft/Phi-4-mini-instruct\",\n",
        "    \"Google Gemma 3\": \"unsloth/gemma-3-4b-it-GGUF\"\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsAESGL0xrKn",
        "outputId": "dae7ef82-6c25-4b73-d1f7-7599c6fbe5f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models available for selection: ['Llama 3.2', 'Microsoft Phi-4 Mini', 'Google Gemma 3', 'Qwen 2.5']\n"
          ]
        }
      ],
      "source": [
        "# --- Global State (or use gr.State in Blocks) ---\n",
        "# To keep track of the currently loaded model/pipeline\n",
        "current_model_id = None\n",
        "current_pipeline = None\n",
        "print(f\"Models available for selection: {list(available_models.keys())}\")\n",
        "\n",
        "\n",
        "# Define a function to Load/Switch Models\n",
        "def load_llm_model(model_name):\n",
        "    \"\"\"Loads the selected LLM, unloading the previous one.\"\"\"\n",
        "    global current_model_id, current_pipeline, tokenizer, model\n",
        "\n",
        "    new_model_id = available_models.get(model_name)\n",
        "    if not new_model_id:\n",
        "        return \"Invalid model selected.\", None  # Return error message and None pipeline\n",
        "\n",
        "    if new_model_id == current_model_id and current_pipeline is not None:\n",
        "        print(f\"Model {model_name} is already loaded.\")\n",
        "        # Indicate success but don't reload\n",
        "        return f\"{model_name} already loaded.\", current_pipeline\n",
        "\n",
        "    print(f\"Switching to model: {model_name} ({new_model_id})...\")\n",
        "\n",
        "    # Unload previous model (important for memory)\n",
        "    # Clear variables and run garbage collection\n",
        "    current_pipeline = None\n",
        "    if \"model\" in locals():\n",
        "        del model\n",
        "    if \"tokenizer\" in locals():\n",
        "        del tokenizer\n",
        "    if \"pipe\" in locals():\n",
        "        del pipe\n",
        "    torch.cuda.empty_cache()  # Clear GPU memory cache\n",
        "    import gc\n",
        "\n",
        "    gc.collect()\n",
        "    print(\"Previous model unloaded (if any).\")\n",
        "\n",
        "    # --- Load the new model ---\n",
        "    loading_message = f\"Loading {model_name}...\"\n",
        "    try:\n",
        "        # Load Tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(new_model_id, trust_remote_code = True)\n",
        "\n",
        "        # Load Model (Quantized)\n",
        "        model = AutoModelForCausalLM.from_pretrained(new_model_id,\n",
        "                                                     torch_dtype = \"auto\",  # \"torch.float16\", # Or bfloat16 if available\n",
        "                                                     load_in_4bit = True,\n",
        "                                                     device_map = \"auto\",\n",
        "                                                     trust_remote_code = True)\n",
        "\n",
        "        # Create Pipeline\n",
        "        loaded_pipeline = pipeline(\n",
        "            \"text-generation\", model = model, tokenizer = tokenizer, torch_dtype = \"auto\", device_map = \"auto\")\n",
        "\n",
        "        print(f\"Model {model_name} loaded successfully!\")\n",
        "        current_model_id = new_model_id\n",
        "        current_pipeline = loaded_pipeline  # Update global state\n",
        "        # Use locals() or return values with gr.State for better Gradio practice\n",
        "        return f\"{model_name} loaded successfully!\", loaded_pipeline  # Status message and the pipeline object\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model {model_name}: {e}\")\n",
        "        current_model_id = None\n",
        "        current_pipeline = None\n",
        "        return f\"Error loading {model_name}: {e}\", None  # Error message and None pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc48bpwpxCvj"
      },
      "outputs": [],
      "source": [
        "# --- Function to handle Q&A Submission ---\n",
        "# This function now relies on the globally managed 'current_pipeline'\n",
        "# In a more robust Gradio app, you'd pass the pipeline via gr.State\n",
        "def handle_submit(question):\n",
        "    \"\"\"Handles the user submitting a question.\"\"\"\n",
        "    if not current_pipeline:\n",
        "        return \"Error: No model is currently loaded. Please select a model.\"\n",
        "    if not pdf_text:\n",
        "        return \"Error: PDF text is not loaded. Please run Section 4.\"\n",
        "    if not question:\n",
        "        return \"Please enter a question.\"\n",
        "\n",
        "    print(f\"Handling submission for question: '{question}' using {current_model_id}\")\n",
        "    # Call the Q&A function defined in Section 5\n",
        "    answer = answer_question_from_pdf(pdf_text, question, current_pipeline)\n",
        "    return answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "S0whY_V9xrKo",
        "outputId": "db747f41-1b93-4f5f-949b-3cc6739b30cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Gradio interface...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gr' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-885397300.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- Build Gradio Interface using Blocks ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building Gradio interface...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     gr.Markdown(\n\u001b[1;32m      5\u001b[0m         f\"\"\"\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gr' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Build Gradio Interface using Blocks ---\n",
        "print(\"Building Gradio interface...\")\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\n",
        "        f\"\"\"\n",
        "    # PDF Q&A Bot Using Hugging Face Open-Source Models\n",
        "    Ask questions about the document ('{pdf_filename}' if loaded, {len(pdf_text)} chars).\n",
        "    Select an open-source LLM to answer your question.\n",
        "    **Note:** Switching models takes time as the new model needs to be downloaded and loaded into the GPU.\n",
        "    \"\"\"\n",
        "    )\n",
        "\n",
        "    # Store the pipeline in Gradio state for better practice (optional for this simple version)\n",
        "    # llm_pipeline_state = gr.State(None)\n",
        "\n",
        "    with gr.Row():\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            choices=list(available_models.keys()),\n",
        "            label=\"ü§ñ Select LLM Model\",\n",
        "            value=list(available_models.keys())[0],  # Default to the first model\n",
        "        )\n",
        "        status_textbox = gr.Textbox(label=\"Model Status\", interactive=False)\n",
        "\n",
        "    question_textbox = gr.Textbox(\n",
        "        label=\"‚ùì Your Question\", lines=2, placeholder=\"Enter your question about the document here...\"\n",
        "    )\n",
        "    submit_button = gr.Button(\"Submit Question\", variant=\"primary\")\n",
        "    answer_textbox = gr.Textbox(label=\"üí° Answer\", lines=5, interactive=False)\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "    # When the dropdown changes, load the selected model\n",
        "    model_dropdown.change(\n",
        "        fn = load_llm_model,\n",
        "        inputs = [model_dropdown],\n",
        "        outputs = [status_textbox],  # Update status text. Ideally also update a gr.State for the pipeline\n",
        "        # outputs=[status_textbox, llm_pipeline_state] # If using gr.State\n",
        "    )\n",
        "\n",
        "    # When the button is clicked, call the submit handler\n",
        "    submit_button.click(\n",
        "        fn = handle_submit,\n",
        "        inputs = [question_textbox],\n",
        "        outputs = [answer_textbox],\n",
        "        # inputs=[question_textbox, llm_pipeline_state], # Pass state if using it\n",
        "    )\n",
        "\n",
        "    # --- Initial Model Load ---\n",
        "    # Easier: Manually load first model *before* launching Gradio for simplicity here\n",
        "    initial_model_name = list(available_models.keys())[0]\n",
        "    print(f\"Performing initial load of default model: {initial_model_name}...\")\n",
        "    status, _ = load_llm_model(initial_model_name)\n",
        "    status_textbox.value = status  # Set initial status\n",
        "    print(\"Initial load complete.\")\n",
        "\n",
        "\n",
        "# --- Launch the Gradio App ---\n",
        "print(\"Launching Gradio demo...\")\n",
        "demo.launch(debug=True)  # debug=True provides more detailed logs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eaacaa044fbb4d41a429aded65f1d598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_eb2ecc44a404404dae0e88ac81ee30e9"
          }
        },
        "35698f748c634bfd8a8375dbcdd2b5bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd54263dd4874d4d8762eb3b71112170",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2558f22ce7064557ba9b1eb8f7d9504a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "b7860e5a0f42464db5bbf2cb3f13bfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_fe4c873e5a174b328928529b8926d4d9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5a971d4c8cb547afb2c91c0b3d6ba6e9",
            "value": ""
          }
        },
        "9940891b55664eb7915cba801d9e5d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_02568b198c994b4cbf309ba57651797a",
            "style": "IPY_MODEL_9734b9bce11e4ca29423e5842c560bda",
            "value": true
          }
        },
        "583587adb2fa43fc8113c65327011d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4b17a157a57e4c3c9d9f9b3978fa0d48",
            "style": "IPY_MODEL_a1ca5b145ec14b3fba77d1848bbeb957",
            "tooltip": ""
          }
        },
        "1dcf3a096aee4d80bf60f7ab2523dfd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef1072e844a74341b83355f55e0b9ecf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fe00368e5c43489fb5d64a266da020e1",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "eb2ecc44a404404dae0e88ac81ee30e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fd54263dd4874d4d8762eb3b71112170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2558f22ce7064557ba9b1eb8f7d9504a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe4c873e5a174b328928529b8926d4d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a971d4c8cb547afb2c91c0b3d6ba6e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02568b198c994b4cbf309ba57651797a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9734b9bce11e4ca29423e5842c560bda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b17a157a57e4c3c9d9f9b3978fa0d48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1ca5b145ec14b3fba77d1848bbeb957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ef1072e844a74341b83355f55e0b9ecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe00368e5c43489fb5d64a266da020e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05f51f3ea8e94bfb89c5354945ffc821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59b383c7793546abb87ab5d28234dfc4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e75a70e169a44465bc938a0741684030",
            "value": "Connecting..."
          }
        },
        "59b383c7793546abb87ab5d28234dfc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e75a70e169a44465bc938a0741684030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}